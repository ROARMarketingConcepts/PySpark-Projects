{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kkSXsFae8Cct",
        "7AYS0MlUvfrI",
        "WNaboQo22HOW",
        "2fj4XecA5nX_",
        "fUjc4g4xHE7p",
        "D6Syynf5QCxB",
        "0k8qNqs2T2a0",
        "jzlxJnZ6bNcq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "#Getting Started with PySpark in Google Colab\n",
        "\n",
        "PySpark is Python interface for Apache Spark. The primary use cases for PySpark are to work with huge amounts of data and for creating data pipelines.\n",
        "\n",
        "You don't need to work with big data to benefit from PySpark. I find that the SparkSQL is a great tool for performing routine data anlysis. Pandas can get slow and you may find yourself writing a lot of code for data cleaning whereas the same actions take much less code in SQL. Let's get started!\n",
        "\n",
        "See more here! http://spark.apache.org/docs/latest/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "hxv7w_2y2bb9",
        "outputId": "2f31a371-0c0a-4539-e09e-cc710f4e6648"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"PySpark Book Ch. 2\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,665 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,926 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,211 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.2 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,720 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,901 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,546 kB]\n",
            "Fetched 22.7 MB in 3s (7,840 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "34 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e21f5318b50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://19fe21e1e48f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark Book Ch. 2</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "hcOCBgQo2Pqf",
        "outputId": "63150a8c-102a-43f5-b10c-243b421f1eb5"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e21f5318b50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://19fe21e1e48f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark Book Ch. 2</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# 2. Reading Data\n",
        "\n",
        "For this example, I am going to use a publicly available data set in a CSV format."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NWs9j1Yk3mu2",
        "outputId": "79094682-d0df-424a-ddce-cbad7317f889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzvNxiQSixRU"
      },
      "source": [
        "# import requests\n",
        "# path = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "# req = requests.get(path)\n",
        "# url_content = req.content\n",
        "\n",
        "# csv_file_name = 'owid-covid-data.csv'\n",
        "# csv_file = open(csv_file_name, 'wb')\n",
        "\n",
        "# csv_file.write(url_content)\n",
        "# csv_file.close()\n",
        "file_path = '/content/drive/MyDrive/Colab_Notebooks/PySpark Data Analysis/Advanced Analytics With PySpark - OReilly/donation/'\n",
        "\n",
        "df = spark.read.csv(file_path+'block_*.csv', header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRUC46L_8zX"
      },
      "source": [
        "#3. PySpark DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-PgzP3IjZsV",
        "outputId": "9ee8ad77-ae58-419d-92e5-0daf69e236ae"
      },
      "source": [
        "#Viewing the dataframe schema\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id_1: integer (nullable = true)\n",
            " |-- id_2: integer (nullable = true)\n",
            " |-- cmp_fname_c1: string (nullable = true)\n",
            " |-- cmp_fname_c2: string (nullable = true)\n",
            " |-- cmp_lname_c1: double (nullable = true)\n",
            " |-- cmp_lname_c2: string (nullable = true)\n",
            " |-- cmp_sex: integer (nullable = true)\n",
            " |-- cmp_bd: string (nullable = true)\n",
            " |-- cmp_bm: string (nullable = true)\n",
            " |-- cmp_by: string (nullable = true)\n",
            " |-- cmp_plz: string (nullable = true)\n",
            " |-- is_match: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "Cuttc9iqtmSQ",
        "outputId": "1c81cd6b-5cbe-4e27-866b-87d91b36ac8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| 3148| 8326|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|14055|94934|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|33948|34740|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|  946|71870|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|64880|71676|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|25739|45991|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|62415|93584|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      0|    true|\n",
            "|27995|31399|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "| 4909|12238|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|15161|16743|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|31703|37310|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|30213|36558|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|56596|56630|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|16481|21174|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|32649|37094|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|34268|37260|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|66117|69253|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      0|    true|\n",
            "| 2771|31982|           1|           ?|         1.0|           ?|      0|     1|     1|     1|      1|    true|\n",
            "|23557|29673|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "|37156|39557|           1|           ?|         1.0|           ?|      1|     1|     1|     1|      1|    true|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace('?','')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbb-XsbDM0o_",
        "outputId": "aff3fade-2ade-433b-9f9d-32823736d90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| 3148| 8326|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|14055|94934|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|33948|34740|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|  946|71870|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|64880|71676|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|25739|45991|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|62415|93584|           1|            |         1.0|            |      1|     1|     1|     1|      0|    true|\n",
            "|27995|31399|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "| 4909|12238|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|15161|16743|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|31703|37310|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|30213|36558|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|56596|56630|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|16481|21174|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|32649|37094|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|34268|37260|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|66117|69253|           1|            |         1.0|            |      1|     1|     1|     1|      0|    true|\n",
            "| 2771|31982|           1|            |         1.0|            |      0|     1|     1|     1|      1|    true|\n",
            "|23557|29673|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|37156|39557|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E2ufR3DNHzh",
        "outputId": "e91941fd-f9de-4188-ada4-aac6a9658f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id_1: integer (nullable = true)\n",
            " |-- id_2: integer (nullable = true)\n",
            " |-- cmp_fname_c1: string (nullable = true)\n",
            " |-- cmp_fname_c2: string (nullable = true)\n",
            " |-- cmp_lname_c1: double (nullable = true)\n",
            " |-- cmp_lname_c2: string (nullable = true)\n",
            " |-- cmp_sex: integer (nullable = true)\n",
            " |-- cmp_bd: string (nullable = true)\n",
            " |-- cmp_bm: string (nullable = true)\n",
            " |-- cmp_by: string (nullable = true)\n",
            " |-- cmp_plz: string (nullable = true)\n",
            " |-- is_match: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TubPOhgDDEsV",
        "outputId": "cce7d7ca-ac8c-4a09-9a35-ffcfa6032e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5749132"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ylA4B2kfd2",
        "outputId": "506835f5-d851-4b25-b483-3d778aac6c17"
      },
      "source": [
        "#Summary stats\n",
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
            "|summary|              id_1|              id_2|      cmp_fname_c1|      cmp_fname_c2|       cmp_lname_c1|       cmp_lname_c2|            cmp_sex|             cmp_bd|             cmp_bm|            cmp_by|            cmp_plz|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
            "|  count|           5749132|           5749132|           5749132|           5749132|            5749132|            5749132|            5749132|            5749132|            5749132|           5749132|            5749132|\n",
            "|   mean| 33324.48559643438| 66587.43558331935|0.7129024704425707| 0.900017671890335|0.31562781930763056|0.31841283153174366|  0.955001381078048|0.22446526708507172|0.48885529849763504|0.2227485966810923|0.00552866147434343|\n",
            "| stddev|23659.859374487987|23620.487613269706|0.3887583596162788|0.2713176105782331| 0.3342336339615803| 0.3685670662006655|0.20730111116897443|0.41722972238461925| 0.4998758236779003|0.4160909629831711|0.07414914925420013|\n",
            "|    min|                 1|                 6|                  |                  |                0.0|                   |                  0|                   |                   |                  |                   |\n",
            "|    max|             99980|            100000|                 1|                 1|                1.0|                  1|                  1|                  1|                  1|                 1|                  1|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuuP5a5uEwBe",
        "outputId": "c096ea96-5cec-4d8b-f20f-0e664e038285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id_1: int, id_2: int, cmp_fname_c1: string, cmp_fname_c2: string, cmp_lname_c1: double, cmp_lname_c2: string, cmp_sex: int, cmp_bd: string, cmp_bm: string, cmp_by: string, cmp_plz: string, is_match: boolean]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the value counts for the `is_match` column..."
      ],
      "metadata": {
        "id": "RtmAS5d4E7UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('is_match').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u6S1BgiFDrg",
        "outputId": "c4225aea-60ba-412b-f17b-2623a3a9e111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+\n",
            "|is_match|  count|\n",
            "+--------+-------+\n",
            "|    true|  20931|\n",
            "|   false|5728201|\n",
            "+--------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg(F.avg('cmp_sex'),F.stddev('cmp_sex')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIY_XR6_Fb_C",
        "outputId": "2ed77264-2050-40ba-8164-aad7bd99d77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+\n",
            "|     avg(cmp_sex)|    stddev(cmp_sex)|\n",
            "+-----------------+-------------------+\n",
            "|0.955001381078048|0.20730111116897443|\n",
            "+-----------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6e41cFEFSR"
      },
      "source": [
        "# 4. Spark SQL\n",
        "\n",
        "What I really like about the SQL module is that it's very approachable to interact with your data while still using Spark. There is less to learn since it's basically the same SQL syntax you might already be comfortable with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBpoPIGDrb-c"
      },
      "source": [
        "#Creating a table from the dataframe\n",
        "df.createOrReplaceTempView(\"some_random_data\") #temporary view\n",
        "# df.saveAsTable(\"covid_data\") #Save as a table\n",
        "# df.write.mode(\"overwrite\").saveAsTable(\"covid_data\") #Save as table and overwrite table if exits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFcoi5l7kyLq",
        "outputId": "e2414aab-7b4e-4d11-ea36-e73f1fe72075"
      },
      "source": [
        "df2 = spark.sql(\"SELECT * from some_random_data WHERE is_match='true'\")\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id_1: integer (nullable = true)\n",
            " |-- id_2: integer (nullable = true)\n",
            " |-- cmp_fname_c1: string (nullable = true)\n",
            " |-- cmp_fname_c2: string (nullable = true)\n",
            " |-- cmp_lname_c1: double (nullable = true)\n",
            " |-- cmp_lname_c2: string (nullable = true)\n",
            " |-- cmp_sex: integer (nullable = true)\n",
            " |-- cmp_bd: string (nullable = true)\n",
            " |-- cmp_bm: string (nullable = true)\n",
            " |-- cmp_by: string (nullable = true)\n",
            " |-- cmp_plz: string (nullable = true)\n",
            " |-- is_match: boolean (nullable = true)\n",
            "\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "| 3148| 8326|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|14055|94934|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|33948|34740|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|  946|71870|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|64880|71676|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|25739|45991|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|62415|93584|           1|            |         1.0|            |      1|     1|     1|     1|      0|    true|\n",
            "|27995|31399|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "| 4909|12238|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|15161|16743|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|31703|37310|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|30213|36558|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|56596|56630|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|16481|21174|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|32649|37094|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|34268|37260|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|66117|69253|           1|            |         1.0|            |      1|     1|     1|     1|      0|    true|\n",
            "| 2771|31982|           1|            |         1.0|            |      0|     1|     1|     1|      1|    true|\n",
            "|23557|29673|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "|37156|39557|           1|            |         1.0|            |      1|     1|     1|     1|      1|    true|\n",
            "+-----+-----+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTvI4jbZjX31"
      },
      "source": [
        "# 5. Example with Another Data Set\n",
        "This data set comes with your Google Colab Session"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdUpJ_KVQ46I",
        "outputId": "f87ba6b1-4e7d-4ede-ab83-5930081b0159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/camnugent/california-housing-prices?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400k/400k [00:00<00:00, 45.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/camnugent/california-housing-prices/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "nTuktVe9Rd1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-To1oW2S4mZL"
      },
      "source": [
        "# df = spark.read.csv(\"/content/sample_data/california_housing_train.csv\", header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvDrXp8w4pFi",
        "outputId": "349bb6b2-d24b-485e-dfcd-db74830f3104"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- housing_median_age: double (nullable = true)\n",
            " |-- total_rooms: double (nullable = true)\n",
            " |-- total_bedrooms: double (nullable = true)\n",
            " |-- population: double (nullable = true)\n",
            " |-- households: double (nullable = true)\n",
            " |-- median_income: double (nullable = true)\n",
            " |-- median_house_value: double (nullable = true)\n",
            " |-- ocean_proximity: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-ra4P9Z7sut",
        "outputId": "a40353d0-1200-45f2-e0e2-454e7a77c8b9"
      },
      "source": [
        "#print N rows\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
            "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
            "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
            "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
            "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKzuJaaw79Kf",
        "outputId": "b7460987-1ec4-461e-b915-af5be70c6555"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20640"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('ocean_proximity').agg(F.mean('median_house_value')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS1Vg7mb2lRh",
        "outputId": "75d8ca6d-5ac2-443f-987b-f89845625ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------------------+\n",
            "|ocean_proximity|avg(median_house_value)|\n",
            "+---------------+-----------------------+\n",
            "|         ISLAND|               380440.0|\n",
            "|     NEAR OCEAN|     249433.97742663656|\n",
            "|       NEAR BAY|     259212.31179039303|\n",
            "|      <1H OCEAN|     240084.28546409807|\n",
            "|         INLAND|     124805.39200122119|\n",
            "+---------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvjbab-J7_t_",
        "outputId": "4911b9c8-b98a-476d-b908-d52342ab3dba"
      },
      "source": [
        "df.select(\"housing_median_age\",\"total_rooms\").show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------+\n",
            "|housing_median_age|total_rooms|\n",
            "+------------------+-----------+\n",
            "|              41.0|      880.0|\n",
            "|              21.0|     7099.0|\n",
            "|              52.0|     1467.0|\n",
            "|              52.0|     1274.0|\n",
            "|              52.0|     1627.0|\n",
            "+------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAzEcJp78NIH",
        "outputId": "7851108d-eddb-4deb-a85e-7d9d17bed4ee"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
            "|summary|          longitude|         latitude|housing_median_age|       total_rooms|    total_bedrooms|        population|       households|     median_income|median_house_value|ocean_proximity|\n",
            "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
            "|  count|              20640|            20640|             20640|             20640|             20433|             20640|            20640|             20640|             20640|          20640|\n",
            "|   mean|-119.56970445736148| 35.6318614341087|28.639486434108527|2635.7630813953488| 537.8705525375618|1425.4767441860465|499.5396802325581|3.8706710029070246|206855.81690891474|           NULL|\n",
            "| stddev|  2.003531723502584|2.135952397457101| 12.58555761211163|2181.6152515827944|421.38507007403115|  1132.46212176534|382.3297528316098| 1.899821717945263|115395.61587441359|           NULL|\n",
            "|    min|            -124.35|            32.54|               1.0|               2.0|               1.0|               3.0|              1.0|            0.4999|           14999.0|      <1H OCEAN|\n",
            "|    max|            -114.31|            41.95|              52.0|           39320.0|            6445.0|           35682.0|           6082.0|           15.0001|          500001.0|     NEAR OCEAN|\n",
            "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl85UrLC8PYW",
        "outputId": "2fb400f4-87b9-43bf-fa95-cc3bfd2d8ded"
      },
      "source": [
        "df.select('total_rooms').distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|total_rooms|\n",
            "+-----------+\n",
            "|      769.0|\n",
            "|     2862.0|\n",
            "|     3980.0|\n",
            "|     1761.0|\n",
            "|      692.0|\n",
            "|     4800.0|\n",
            "|      496.0|\n",
            "|      934.0|\n",
            "|     2734.0|\n",
            "|     1051.0|\n",
            "|      299.0|\n",
            "|     2815.0|\n",
            "|     4066.0|\n",
            "|     7554.0|\n",
            "|     5776.0|\n",
            "|      305.0|\n",
            "|     6433.0|\n",
            "|    12467.0|\n",
            "|     5983.0|\n",
            "|     4142.0|\n",
            "+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5tOUkPJ8XRb"
      },
      "source": [
        "test = df.groupBy('total_rooms').agg(F.sum('housing_median_age'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv6f3pGb_XR8"
      },
      "source": [
        "test.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaUs3Q-F8lA3"
      },
      "source": [
        "#Counting and removing missing values\n",
        "\n",
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4zy4gdbLnE"
      },
      "source": [
        "# Creating a Test Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "750m7yL29U4g"
      },
      "source": [
        "data = [\n",
        "        ('John','Smith',1),\n",
        "        ('Jane','Smith',2),\n",
        "        ('Jonas','Smith',3),\n",
        "]\n",
        "\n",
        "# columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "columns = [\"firstname\",\"lastname\",\"id\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17sJ9jaliV-b"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbsONR7IRZnl"
      },
      "source": [
        "# Spark Tips and Tricks\n",
        "\n",
        "This is a collection of code snippets for common or tricky tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas DataFrame to Spark DataFrame"
      ],
      "metadata": {
        "id": "kkSXsFae8Cct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(np.random.randint(100,size=(1000, 3)),columns=['A','B','C'])\n",
        "spark_df = spark.createDataFrame(df)\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "858-wUBy8BhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Object columns in pandas dataframe to a string\n",
        "for i in df.select_dtypes(include='object').columns.tolist():\n",
        "\tdf[i] = df[i].astype(str)\n",
        "\n",
        "#Convert datetimes to UTC\n",
        "for i in [col for col in df.columns if df[col].dtype == 'datetime64[ns]']:\n",
        "  df[i] = pd.to_datetime(df[i], utc=True)\n",
        "\n",
        "#Replace nan and \"None\" in pandas dataframe to null in the spark dataframe\n",
        "spark_df = spark.createDataFrame(df).replace('None', None).replace(float('nan'), None)"
      ],
      "metadata": {
        "id": "M3C2b3PX-7FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGa0hwxCRlEf"
      },
      "source": [
        "##Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIL2p4P3Z6bu"
      },
      "source": [
        "data = [\n",
        "        (1,'2021-01-01 10:00:00'),\n",
        "        (1,'2021-01-01 11:00:00'),\n",
        "        (1,'2021-01-01 12:00:00'),\n",
        "        (2,'2021-01-01 12:00:00'),\n",
        "        (2,'2021-01-01 13:00:00'),\n",
        "        (2,'2021-01-01 14:00:00'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"datetime\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ZCcOiCZ74G"
      },
      "source": [
        "#Selecting the min and max by a specific Group\n",
        "spark.sql('''\n",
        "Select\n",
        "  id,\n",
        "\n",
        "  max(datetime) OVER (Partition BY id ORDER BY datetime) as max_date,\n",
        "  min(datetime) OVER (Partition BY id ORDER BY datetime) as min_date,\n",
        "\n",
        "  ROW_NUMBER() OVER (Partition BY id ORDER BY datetime) as row_number\n",
        "\n",
        "  FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouYaOvMDcYYS"
      },
      "source": [
        "# Selecting the row number or order rank for each row within a specified grouping.\n",
        "# This is great for sub rankings in a table\n",
        "\n",
        "spark.sql('''\n",
        "Select\n",
        "  id,\n",
        "  datetime,\n",
        "\n",
        "  ROW_NUMBER() OVER (Partition BY id ORDER BY datetime) as row_number\n",
        "\n",
        "  FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## De-duplicate data by returning the most recently updated row using a window function"
      ],
      "metadata": {
        "id": "eJgMqCiJGfZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "        (1,'2021-01-01',100,'A'),\n",
        "        (1,'2021-01-31',105,'A'),\n",
        "        (2,'2021-02-04',160,'B'),\n",
        "        (2,'2021-02-07',145,'B'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"date\",\"score\",\"type\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "JBbA3LY3GwEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.sql(\"\"\"\n",
        "WITH T AS (\n",
        "  SELECT\n",
        "  *,\n",
        "  ROW_NUMBER() OVER (PARTITION BY id ORDER BY date DESC) AS version_number\n",
        "  FROM window_test\n",
        ")\n",
        "\n",
        "SELECT * FROM T WHERE version_number = 1;\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "HwhLxFkibR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "  SELECT\n",
        "  *,\n",
        "  SUM(score) OVER (PARTITION by type ORDER BY date) as score_cumulative\n",
        "  FROM window_test\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "TByf27YO6rdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limit the number of results per group window function"
      ],
      "metadata": {
        "id": "hDNlXaL0QEPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(\n",
        "np.hstack((\n",
        "    np.random.randint(1,5,size=(100000, 1)),\n",
        "    np.random.randint(100,size=(100000, 1))\n",
        "))\n",
        ", columns=['company_id', 'number'])\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"window_test_limits\")\n"
      ],
      "metadata": {
        "id": "pSyMC_ypQIfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "WITH T AS (\n",
        "  SELECT\n",
        "    company_id,\n",
        "    number,\n",
        "    ROW_NUMBER() OVER (PARTITION BY company_id ORDER BY number) AS row_number\n",
        "  FROM window_test_limits\n",
        "    )\n",
        "\n",
        "SELECT * FROM T WHERE row_number <= 100\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "LLSEEW6VQNLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate a 7 day moving average"
      ],
      "metadata": {
        "id": "T0Ynq3qAVWs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pd.date_range('1/1/2022','1/31/2022',freq='D'), columns=['date'])\n",
        "import random\n",
        "df['company_id'] = 1\n",
        "df['number'] = df.apply(lambda x: random.randint(0,100), axis = 1)\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"window_data\")\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "e1Rkc0bCVaoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  date,\n",
        "  company_id,\n",
        "  number,\n",
        "  AVG(number) OVER (PARTITION BY company_id ORDER BY date ASC RANGE BETWEEN INTERVAL 6 DAYS PRECEDING AND CURRENT ROW) as last_7_day_avg\n",
        "FROM window_data\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "atZLJhqIXYIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly Active Users"
      ],
      "metadata": {
        "id": "yjKv-xeZKOdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pd.date_range('1/1/2022','1/31/2022',freq='D'), columns=['login_date'])\n",
        "import random\n",
        "df['company_id'] = 1\n",
        "df['user_id'] = df.apply(lambda x: random.randint(0,3), axis = 1)\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"users_data\")\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "KP7bOaRnKZO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Revisit this transform\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  login_date,\n",
        "  COUNT(user_id) OVER (PARTITION BY login_date ORDER BY login_date ASC RANGE BETWEEN INTERVAL 30 DAYS PRECEDING AND CURRENT ROW) AS monthly_active_users\n",
        "  FROM users_data\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "rtaaCPuZKfTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the time difference between related rows using a window function"
      ],
      "metadata": {
        "id": "BCmuBJ9rGwb-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYfSAEIKRcsQ"
      },
      "source": [
        "data = [\n",
        "        (1,'start','2021-01-01',100,'A'),\n",
        "        (1,'end','2021-01-31',200,'A'),\n",
        "        (2,'start','2021-03-05 4:53:11',100,'A'),\n",
        "        (2,'end','2021-05-01 05:06:38',200,'A'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"session\",\"datetime\",\"station_return\",\"type\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZWxv4Z4WXsE"
      },
      "source": [
        "spark.sql('''\n",
        "SELECT\n",
        "  id,\n",
        "  datetime,\n",
        "  lead(datetime) OVER (PARTITION BY id ORDER BY datetime) as next_datetime,\n",
        "  DATEDIFF(lead(datetime) OVER (PARTITION BY id ORDER BY datetime),datetime) as duration_in_days\n",
        "\n",
        "FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unpivotting"
      ],
      "metadata": {
        "id": "7AYS0MlUvfrI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ--Ii0-bCPt"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "data = [\n",
        "        ('tim', 10, 9, 8, 5),\n",
        "        ('john', 5, 6, 3, 6),\n",
        "        ('jane', 7, 8, 9, 10),\n",
        "\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "   StructField(\"name\", StringType(), True),\n",
        "   StructField(\"experience\", IntegerType(), True),\n",
        "   StructField(\"satisfaction\", IntegerType(), True),\n",
        "   StructField(\"customer_service\", IntegerType(), True),\n",
        "   StructField(\"speed_of_service\", IntegerType(), True)])\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['experience', 'satisfaction', 'customer_service', 'speed_of_service']\n",
        "\n",
        "exprs = f\"\"\"stack({len(cols)}, {\", \".join([f\"'{i}',{i}\" for i in cols])}) as (question,score)\"\"\"\n",
        "\n",
        "unpivotted_df = df.select(\"name\",F.expr(exprs))\n",
        "\n",
        "unpivotted_df.show()"
      ],
      "metadata": {
        "id": "IjeFoWcF9Fqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace Values using a Dictionary"
      ],
      "metadata": {
        "id": "WNaboQo22HOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "    .createDataFrame([\n",
        "        (1, 'hello',3),\n",
        "        (2, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (135246, 'hello',4),\n",
        "        (54936, 'hello',4)\n",
        "        ],\n",
        "        [\"id\", \"text\",\"num\"]))"
      ],
      "metadata": {
        "id": "cYU3M9DnL1am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\n",
        "1: 5555,\n",
        "4:9999\n",
        "}"
      ],
      "metadata": {
        "id": "pFUiVV-V2XHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace(mapping,1,'id').replace(mapping,1,'_3').show()"
      ],
      "metadata": {
        "id": "QsZ_Dx6D5Lxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a Date Range"
      ],
      "metadata": {
        "id": "2fj4XecA5nX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_range_df = spark.sql(\"SELECT explode(sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 day)) as date\")\n",
        "date_range_df.show()"
      ],
      "metadata": {
        "id": "2R8ks2qR5Uh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Concat Row Values after Grouping"
      ],
      "metadata": {
        "id": "VxqgKv6w5x04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "    .createDataFrame([\n",
        "        (1, 'hello',3),\n",
        "        (2, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        ],\n",
        "        [\"id\", \"text\"]))\n",
        "\n",
        "df.createOrReplaceTempView(\"group_array\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "FlcyEw_j9o-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Return every element\n",
        "spark.sql(\"Select g.text, collect_list(g.id) FROM group_array as g GROUP BY 1\").show()"
      ],
      "metadata": {
        "id": "opBXrwRc-KVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Return unique list\n",
        "spark.sql(\"Select g.text, collect_set(g.id) FROM group_array as g GROUP BY 1\").show()"
      ],
      "metadata": {
        "id": "WurXJUP55r4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rename Spark Columns with a Dictionary"
      ],
      "metadata": {
        "id": "4PfBVvnxDhAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_dict = {\n",
        "    'id':'ID',\n",
        "    'test':'hello'\n",
        "}\n",
        "\n",
        "#Select only specific columns from a file\n",
        "df = spark.read.parquet(path).select([k for k in cols_2016])\n",
        "\n",
        "#rename the columns\n",
        "for old_name, new_name in col_dict.items():\n",
        "  df = df.withColumnRenamed(old_name,new_name)\n",
        "\n",
        "df.createOrReplaceTempView(\"test\")\n",
        "\n",
        "test.show()"
      ],
      "metadata": {
        "id": "c5hKJL5Z9hIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read Multiple Parquet Files into one Spark DataFrame"
      ],
      "metadata": {
        "id": "fUjc4g4xHE7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "parquet_files = glob.glob('/content/*.parquet')\n",
        "#The * is a wild card\n",
        "\n",
        "df = spark.read.parquet(*parquet_files)"
      ],
      "metadata": {
        "id": "4UdQCbmCHIGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split and get last element in Spark SQL"
      ],
      "metadata": {
        "id": "D6Syynf5QCxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  \"This.is.a.test\" AS text,\n",
        "  SPLIT(\"This.is.a.test\",'[\\.]') AS split,\n",
        "  REVERSE(SPLIT(\"This.is.a.test\",'[\\.]'))[0] AS last_word\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "nLV1bLK_QIuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling NULL Values"
      ],
      "metadata": {
        "id": "0k8qNqs2T2a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark\n",
        "    .createDataFrame([\n",
        "        (1, 'hello',None),\n",
        "        (2, 'hello',None),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        (3, 'hello',5),\n",
        "        ],\n",
        "        [\"id\", \"text\"]))\n",
        "\n",
        "df.createOrReplaceTempView(\"group_array\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "6zIJ3J1UQfpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"Select * from group_array where _3 IS NOT NULL\").show()"
      ],
      "metadata": {
        "id": "DXsG3qgkUfXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a JDBC Driver"
      ],
      "metadata": {
        "id": "jzlxJnZ6bNcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install JayDeBeApi\n",
        "import jaydebeapi\n",
        "import os\n",
        "\n",
        "#Downlaods JDBC drivers\n",
        "!wget https://repo1.maven.org/maven2/org/apache/hive/hive-jdbc/2.3.7/hive-jdbc-2.3.7-standalone.jar\n",
        "!zip -q -d hive-jdbc-2.3.7-standalone.jar org/apache/logging/log4j/core/lookup/JndiLookup.class\n",
        "!unzip hive-jdbc-2.3.7-standalone.jar > output.txt\n",
        "\n",
        "!wget https://github.com/timveil/hive-jdbc-uber-jar/releases/download/v1.8-2.6.3/hive-jdbc-uber-2.6.3.0-235.jar\n",
        "\n",
        "\n",
        "DRIVER_CLASS = 'org.apache.hive.jdbc.HiveDriver'\n",
        "DRIVER_PATH = 'hive-jdbc-2.3.7-standalone.jar'\n",
        "ASCEND_ENV = 'trial'\n",
        "CONN_URL = 'jdbc:'\n",
        "\n",
        "user = 'admin'\n",
        "pw = 'admin'\n",
        "\n",
        "conn = jdbc.connect(DRIVER_CLASS,\n",
        "                    CONN_URL,\n",
        "                    [user, pw],\n",
        "                    DRIVER_PATH)"
      ],
      "metadata": {
        "id": "UsyjzevaUj4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex"
      ],
      "metadata": {
        "id": "zSv6sT7Rd2O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  '(5) Strongly Agree',\n",
        "  regexp_extract('(10) Strongly Agree', '([0-9]+)')\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "Qeln1haMd3By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zo8SK4fQeMUQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}